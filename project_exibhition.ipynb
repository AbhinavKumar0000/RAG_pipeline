{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRTUvcOuKAQ4eRyWX7oXDw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhinavKumar0000/Project_exhibiton_1/blob/main/project_exibhition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# Replace with your actual API key\n",
        "API_KEY = \\n",
        "\n",
        "try:\n",
        "    genai.configure(api_key=API_KEY)\n",
        "\n",
        "    # Try to list models, which requires a valid API key\n",
        "    for m in genai.list_models():\n",
        "        print(m.name)\n",
        "        break # Exit after finding one model to confirm connection\n",
        "    print(\"API key is valid.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"API key is invalid or an error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ElMMdKlfjs8R",
        "outputId": "f6feacd8-5f5a-4ccd-a651-c8a1537a8b1e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-gecko-001\n",
            "API key is valid.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# Configure your API key\n",
        "genai.configure(api_key=\"AIzaSyAiaNiHISq3C-3makrZBPDMprl0XTPvOXo\")\n",
        "\n",
        "try:\n",
        "    # List all available models\n",
        "    for m in genai.list_models():\n",
        "        print(f\"Model Name: {m.name}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error listing models: {e}\")\n",
        "\n",
        "# The output will show which models are supported, for example:\n",
        "# Model Name: models/gemini-1.5-flash\n",
        "# Model Name: models/gemini-1.5-pro\n",
        "# Model Name: models/gemini-pro-vision-latest\n",
        "\n",
        "# Update your code with a supported model name from the list.\n",
        "# For example, to use gemini-1.5-flash:\n",
        "try:\n",
        "    response = genai.GenerativeModel(model_name='gemini-1.5-flash').generate_content(\"Hello, world!\")\n",
        "    print(\"\\nAPI call successful!\")\n",
        "    print(response.text)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAPI call failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4rb6favUs4E2",
        "outputId": "acbf6d2e-dd60-4d18-fc35-032b3703464a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Name: models/embedding-gecko-001\n",
            "Model Name: models/gemini-1.5-pro-latest\n",
            "Model Name: models/gemini-1.5-pro-002\n",
            "Model Name: models/gemini-1.5-pro\n",
            "Model Name: models/gemini-1.5-flash-latest\n",
            "Model Name: models/gemini-1.5-flash\n",
            "Model Name: models/gemini-1.5-flash-002\n",
            "Model Name: models/gemini-1.5-flash-8b\n",
            "Model Name: models/gemini-1.5-flash-8b-001\n",
            "Model Name: models/gemini-1.5-flash-8b-latest\n",
            "Model Name: models/gemini-2.5-pro-preview-03-25\n",
            "Model Name: models/gemini-2.5-flash-preview-05-20\n",
            "Model Name: models/gemini-2.5-flash\n",
            "Model Name: models/gemini-2.5-flash-lite-preview-06-17\n",
            "Model Name: models/gemini-2.5-pro-preview-05-06\n",
            "Model Name: models/gemini-2.5-pro-preview-06-05\n",
            "Model Name: models/gemini-2.5-pro\n",
            "Model Name: models/gemini-2.0-flash-exp\n",
            "Model Name: models/gemini-2.0-flash\n",
            "Model Name: models/gemini-2.0-flash-001\n",
            "Model Name: models/gemini-2.0-flash-exp-image-generation\n",
            "Model Name: models/gemini-2.0-flash-lite-001\n",
            "Model Name: models/gemini-2.0-flash-lite\n",
            "Model Name: models/gemini-2.0-flash-preview-image-generation\n",
            "Model Name: models/gemini-2.0-flash-lite-preview-02-05\n",
            "Model Name: models/gemini-2.0-flash-lite-preview\n",
            "Model Name: models/gemini-2.0-pro-exp\n",
            "Model Name: models/gemini-2.0-pro-exp-02-05\n",
            "Model Name: models/gemini-exp-1206\n",
            "Model Name: models/gemini-2.0-flash-thinking-exp-01-21\n",
            "Model Name: models/gemini-2.0-flash-thinking-exp\n",
            "Model Name: models/gemini-2.0-flash-thinking-exp-1219\n",
            "Model Name: models/gemini-2.5-flash-preview-tts\n",
            "Model Name: models/gemini-2.5-pro-preview-tts\n",
            "Model Name: models/learnlm-2.0-flash-experimental\n",
            "Model Name: models/gemma-3-1b-it\n",
            "Model Name: models/gemma-3-4b-it\n",
            "Model Name: models/gemma-3-12b-it\n",
            "Model Name: models/gemma-3-27b-it\n",
            "Model Name: models/gemma-3n-e4b-it\n",
            "Model Name: models/gemma-3n-e2b-it\n",
            "Model Name: models/gemini-2.5-flash-lite\n",
            "Model Name: models/embedding-001\n",
            "Model Name: models/text-embedding-004\n",
            "Model Name: models/gemini-embedding-exp-03-07\n",
            "Model Name: models/gemini-embedding-exp\n",
            "Model Name: models/gemini-embedding-001\n",
            "Model Name: models/aqa\n",
            "Model Name: models/imagen-3.0-generate-002\n",
            "Model Name: models/imagen-4.0-generate-preview-06-06\n",
            "Model Name: models/imagen-4.0-ultra-generate-preview-06-06\n",
            "Model Name: models/veo-2.0-generate-001\n",
            "Model Name: models/veo-3.0-generate-preview\n",
            "Model Name: models/veo-3.0-fast-generate-preview\n",
            "Model Name: models/gemini-2.5-flash-preview-native-audio-dialog\n",
            "Model Name: models/gemini-2.5-flash-exp-native-audio-thinking-dialog\n",
            "Model Name: models/gemini-2.0-flash-live-001\n",
            "Model Name: models/gemini-live-2.5-flash-preview\n",
            "Model Name: models/gemini-2.5-flash-live-preview\n",
            "\n",
            "API call successful!\n",
            "Hello there!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "genai.configure(api_key=\"AIzaSyAiaNiHISq3C-3makrZBPDMprl0XTPvOXo\")\n",
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "# Replace with your actual model name, e.g., 'gemini-1.5-pro'\n",
        "MODEL_NAME = 'gemini-1.5-flash'\n",
        "\n",
        "# Set your API key from an environment variable\n",
        "genai.configure(api_key='AIzaSyAiaNiHISq3C-3makrZBPDMprl0XTPvOXo')\n",
        "\n",
        "# --- Main loop ---\n",
        "try:\n",
        "    model = genai.GenerativeModel(MODEL_NAME)\n",
        "\n",
        "    print(f\"Chatbot using model: {MODEL_NAME}\")\n",
        "    print(\"Type 'exit' or 'quit' to end the session.\")\n",
        "\n",
        "    while True:\n",
        "        # Get user input\n",
        "        user_query = input(\"You: \")\n",
        "\n",
        "        # Check for exit commands\n",
        "        if user_query.lower() in ['exit', 'quit']:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Send the query to the Gemini API\n",
        "        response = model.generate_content(user_query)\n",
        "\n",
        "        # Print the response from the model\n",
        "        print(f\"Gemini: {response.text}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "LW9wFFH0thw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "import time\n",
        "import faiss\n",
        "import pickle\n",
        "from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from dotenv import load_dotenv\n",
        "from google.generativeai import list_models, configure"
      ],
      "metadata": {
        "id": "h2ZWyPcn4Xbw"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load environment variables\n",
        "load_dotenv()\n",
        "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
        "    raise EnvironmentError(\"GOOGLE_API_KEY not found in .env file\")"
      ],
      "metadata": {
        "id": "r28_x5E15N3c"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.configure(api_key=\"AIzaSyAiaNiHISq3C-3makrZBPDMprl0XTPvOXo\")"
      ],
      "metadata": {
        "id": "RRj-Ppmv7PD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure Google Generative AI\n",
        "configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
      ],
      "metadata": {
        "id": "curCWjde7Nl2"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check available models\n",
        "def check_available_models():\n",
        "    try:\n",
        "        available_models = [model.name for model in list_models() if \"generateContent\" in model.supported_generation_methods]\n",
        "        return available_models\n",
        "        available_models\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error checking models: {str(e)}\"\n",
        "    return available_models"
      ],
      "metadata": {
        "id": "o25IeXF37fGo"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Gemini LLM and embeddings\n",
        "try:\n",
        "    available_models = check_available_models()\n",
        "    if \"models/gemini-1.5-flash\" not in available_models:\n",
        "        raise ValueError(\"Model gemini-1.5-flash not available. Available models: \" + \", \".join(available_models))\n",
        "    llm = GoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.9, max_output_tokens=500)\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "except Exception as e:\n",
        "    raise Exception(f\"Failed to initialize Gemini: {str(e)}\")"
      ],
      "metadata": {
        "id": "YS34kflB5TNi"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process URLs and create FAISS index\n",
        "def process_urls(url1, url2):\n",
        "    file_path = \"faiss_store_gemini.index\"\n",
        "    metadata_path = \"faiss_store_gemini_metadata.pkl\"\n",
        "    urls = [url for url in [url1, url2] if url and url.strip()]  # Filter out empty or invalid URLs\n",
        "\n",
        "    if not urls:\n",
        "        return \"Please provide at least one valid URL.\"\n",
        "\n",
        "    try:\n",
        "        # Load data\n",
        "        status = \"Data Loading...Started...\"\n",
        "        loader = UnstructuredURLLoader(urls=urls)\n",
        "        data = loader.load()\n",
        "        if not data:\n",
        "            return \"Failed to load data from provided URLs.\"\n",
        "\n",
        "        # Split data\n",
        "        status = \"Text Splitter...Started...\"\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            separators=['\\n\\n', '\\n', '.', ','],\n",
        "            chunk_size=1000\n",
        "        )\n",
        "        docs = text_splitter.split_documents(data)\n",
        "        if not docs:\n",
        "            return \"No documents were extracted from the URLs.\"\n",
        "\n",
        "        # Create embeddings and save to FAISS index\n",
        "        status = \"Embedding Vector Started Building...\"\n",
        "        vectorstore_gemini = FAISS.from_documents(docs, embeddings)\n",
        "        time.sleep(2)\n",
        "\n",
        "        # Save the FAISS index and metadata\n",
        "        faiss.write_index(vectorstore_gemini.index, file_path)\n",
        "        with open(metadata_path, \"wb\") as f:\n",
        "            pickle.dump({\n",
        "                \"docstore\": vectorstore_gemini.docstore,\n",
        "                \"index_to_docstore_id\": vectorstore_gemini.index_to_docstore_id\n",
        "            }, f)\n",
        "\n",
        "        return status\n",
        "    except Exception as e:\n",
        "        return f\"Error processing URLs: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "RgiImRuN5WH3"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to query the FAISS index\n",
        "def query_faiss(question):\n",
        "    file_path = \"faiss_store_gemini.index\"\n",
        "    metadata_path = \"faiss_store_gemini_metadata.pkl\"\n",
        "    if not question or not question.strip():\n",
        "        return \"Please enter a valid question.\", \"\"\n",
        "\n",
        "    if not os.path.exists(file_path) or not os.path.exists(metadata_path):\n",
        "        return \"No FAISS index found. Please process URLs first.\", \"\"\n",
        "\n",
        "    try:\n",
        "        # Load the FAISS index and metadata\n",
        "        index = faiss.read_index(file_path)\n",
        "        with open(metadata_path, \"rb\") as f:\n",
        "            metadata = pickle.load(f)\n",
        "\n",
        "        # Reconstruct the FAISS vectorstore\n",
        "        vectorstore = FAISS(\n",
        "            embedding_function=embeddings,\n",
        "            index=index,\n",
        "            docstore=metadata[\"docstore\"],\n",
        "            index_to_docstore_id=metadata[\"index_to_docstore_id\"]\n",
        "        )\n",
        "\n",
        "        # Query the vectorstore\n",
        "        chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorstore.as_retriever())\n",
        "        result = chain({\"question\": question}, return_only_outputs=True)\n",
        "\n",
        "        answer = result[\"answer\"]\n",
        "        sources = result.get(\"sources\", \"\")\n",
        "        sources_output = \"Sources:\\n\" + \"\\n\".join(sources.split(\"\\n\")) if sources else \"No sources available.\"\n",
        "\n",
        "        return answer, sources_output\n",
        "    except Exception as e:\n",
        "        return f\"Error querying FAISS index: {str(e)}\", \"\"\n"
      ],
      "metadata": {
        "id": "DcoBkfjO5Y73"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio Interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# WebInsight Querry\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"## News Article URLs\")\n",
        "            url1 = gr.Textbox(label=\"URL 1\", placeholder=\"Enter a valid news article URL\")\n",
        "            url2 = gr.Textbox(label=\"URL 2\", placeholder=\"Enter a valid news article URL\")\n",
        "            process_button = gr.Button(\"Process URLs\")\n",
        "            status_output = gr.Textbox(label=\"Status\", lines=2)\n",
        "\n",
        "        with gr.Column():\n",
        "            question_input = gr.Textbox(label=\"Question\", placeholder=\"Ask a question about the articles\")\n",
        "            answer_output = gr.Textbox(label=\"Answer\", lines=5)\n",
        "            sources_output = gr.Textbox(label=\"Sources\", lines=5)\n",
        "            query_button = gr.Button(\"Submit Question\")\n",
        "\n",
        "    process_button.click(\n",
        "        fn=process_urls,\n",
        "        inputs=[url1, url2],\n",
        "        outputs=status_output\n",
        "    )\n",
        "\n",
        "    query_button.click(\n",
        "        fn=query_faiss,\n",
        "        inputs=question_input,\n",
        "        outputs=[answer_output, sources_output]\n",
        "    )\n",
        "\n",
        "    # Allow Enter key to submit question\n",
        "    question_input.submit(\n",
        "        fn=query_faiss,\n",
        "        inputs=question_input,\n",
        "        outputs=[answer_output, sources_output]\n",
        "    )\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "vw99wxbS5bzJ",
        "outputId": "3763f5c5-2220-4e37-b230-ef7deb9ce0ca"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://970442cd5cc92ff5e5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://970442cd5cc92ff5e5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ]
}
